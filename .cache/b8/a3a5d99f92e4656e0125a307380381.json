{"id":"camera.js","dependencies":[{"name":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/package.json","includedInParent":true,"mtime":1636789466000},{"name":"@tensorflow-models/posenet","loc":{"line":18,"column":32},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/@tensorflow-models/posenet/dist/posenet.esm.js"},{"name":"@tensorflow-models/facemesh","loc":{"line":19,"column":33},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/@tensorflow-models/facemesh/dist/facemesh.esm.js"},{"name":"@tensorflow/tfjs","loc":{"line":20,"column":20},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/@tensorflow/tfjs/dist/tf.esm.js"},{"name":"paper","loc":{"line":21,"column":23},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/paper/dist/paper-full.js"},{"name":"dat.gui","loc":{"line":22,"column":16},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/dat.gui/build/dat.gui.module.js"},{"name":"stats.js","loc":{"line":23,"column":18},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/stats.js/build/stats.min.js"},{"name":"babel-polyfill","loc":{"line":24,"column":7},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/node_modules/babel-polyfill/lib/index.js"},{"name":"./utils/demoUtils","loc":{"line":26,"column":95},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/utils/demoUtils.js"},{"name":"./utils/svgUtils","loc":{"line":27,"column":23},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/utils/svgUtils.js"},{"name":"./illustrationGen/illustration","loc":{"line":28,"column":31},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/illustrationGen/illustration.js"},{"name":"./illustrationGen/skeleton","loc":{"line":29,"column":43},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/illustrationGen/skeleton.js"},{"name":"./utils/fileUtils","loc":{"line":30,"column":24},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/utils/fileUtils.js"},{"name":"./resources/illustration/amoeba.svg","loc":{"line":33,"column":27},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/amoeba.svg"},{"name":"./resources/illustration/bird.svg","loc":{"line":34,"column":25},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/bird.svg"},{"name":"./resources/illustration/elephant.svg","loc":{"line":35,"column":29},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/elephant.svg"},{"name":"./resources/illustration/fish.svg","loc":{"line":36,"column":25},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/fish.svg"},{"name":"./resources/illustration/frog.svg","loc":{"line":37,"column":25},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/frog.svg"},{"name":"./resources/illustration/mushroom.svg","loc":{"line":38,"column":29},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/mushroom.svg"},{"name":"./resources/illustration/panther.svg","loc":{"line":39,"column":28},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/panther.svg"},{"name":"./resources/illustration/penguin.svg","loc":{"line":40,"column":28},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/penguin.svg"},{"name":"./resources/illustration/toad.svg","loc":{"line":41,"column":25},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/toad.svg"},{"name":"./resources/illustration/tree.svg","loc":{"line":42,"column":25},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/illustration/tree.svg"},{"name":"./resources/music/bird.mp3","loc":{"line":45,"column":27},"parent":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/camera.js","resolved":"/Users/kmadasu/kishore/trainings/javascript-gt/gautam/poseanimator/resources/music/bird.mp3"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.bindPage = bindPage;\n\nvar posenet_module = _interopRequireWildcard(require(\"@tensorflow-models/posenet\"));\n\nvar facemesh_module = _interopRequireWildcard(require(\"@tensorflow-models/facemesh\"));\n\nvar tf = _interopRequireWildcard(require(\"@tensorflow/tfjs\"));\n\nvar paper = _interopRequireWildcard(require(\"paper\"));\n\nvar _dat = _interopRequireDefault(require(\"dat.gui\"));\n\nvar _stats = _interopRequireDefault(require(\"stats.js\"));\n\nrequire(\"babel-polyfill\");\n\nvar _demoUtils = require(\"./utils/demoUtils\");\n\nvar _svgUtils = require(\"./utils/svgUtils\");\n\nvar _illustration = require(\"./illustrationGen/illustration\");\n\nvar _skeleton = require(\"./illustrationGen/skeleton\");\n\nvar _fileUtils = require(\"./utils/fileUtils\");\n\nvar amoebaSVG = _interopRequireWildcard(require(\"./resources/illustration/amoeba.svg\"));\n\nvar birdSVG = _interopRequireWildcard(require(\"./resources/illustration/bird.svg\"));\n\nvar elephantSVG = _interopRequireWildcard(require(\"./resources/illustration/elephant.svg\"));\n\nvar fishSVG = _interopRequireWildcard(require(\"./resources/illustration/fish.svg\"));\n\nvar frogSVG = _interopRequireWildcard(require(\"./resources/illustration/frog.svg\"));\n\nvar mushroomSVG = _interopRequireWildcard(require(\"./resources/illustration/mushroom.svg\"));\n\nvar pantherSVG = _interopRequireWildcard(require(\"./resources/illustration/panther.svg\"));\n\nvar penguinSVG = _interopRequireWildcard(require(\"./resources/illustration/penguin.svg\"));\n\nvar toadSVG = _interopRequireWildcard(require(\"./resources/illustration/toad.svg\"));\n\nvar treeSVG = _interopRequireWildcard(require(\"./resources/illustration/tree.svg\"));\n\nvar birdMusic = _interopRequireWildcard(require(\"./resources/music/bird.mp3\"));\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache() { if (typeof WeakMap !== \"function\") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }\n\nfunction _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"next\", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"throw\", err); } _next(undefined); }); }; }\n\n// Camera stream video element\nvar video;\nvar videoWidth = 300;\nvar videoHeight = 300; // Canvas\n\nvar faceDetection = null;\nvar illustration = null;\nvar canvasScope;\nvar canvasWidth = 800;\nvar canvasHeight = 800; // ML models\n\nvar facemesh;\nvar posenet;\nvar minPoseConfidence = 0.15;\nvar minPartConfidence = 0.1;\nvar nmsRadius = 30.0; // Misc\n\nvar mobile = false;\nvar stats = new _stats.default();\nvar avatarSvgs = {\n  'amoeba': amoebaSVG.default,\n  'bird': birdSVG.default,\n  'elephant': elephantSVG.default,\n  'fish': fishSVG.default,\n  'frog': frogSVG.default,\n  'mushroom': mushroomSVG.default,\n  'panther': pantherSVG.default,\n  'penguin': penguinSVG.default,\n  'toad': toadSVG.default,\n  'tree': treeSVG.default\n};\nvar svgItems = [];\nvar audio = new Audio(birdMusic); // let audio = document.getElementById(birdMusic)\n\nfunction getRandomInt(min, max) {\n  min = Math.ceil(min);\n  max = Math.floor(max);\n  return Math.floor(Math.random() * (max - min + 1)) + min;\n}\n\nfunction playAudio() {\n  audio.play();\n}\n/**\n * Loads a the camera to be used in the demo\n *\n */\n\n\nfunction setupCamera() {\n  return _setupCamera.apply(this, arguments);\n}\n\nfunction _setupCamera() {\n  _setupCamera = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee2() {\n    var video, stream;\n    return regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            if (!(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia)) {\n              _context2.next = 2;\n              break;\n            }\n\n            throw new Error('Browser API navigator.mediaDevices.getUserMedia not available');\n\n          case 2:\n            video = document.getElementById('video');\n            video.width = videoWidth;\n            video.height = videoHeight;\n            _context2.next = 7;\n            return navigator.mediaDevices.getUserMedia({\n              'audio': false,\n              'video': {\n                facingMode: 'user',\n                width: videoWidth,\n                height: videoHeight\n              }\n            });\n\n          case 7:\n            stream = _context2.sent;\n            video.srcObject = stream;\n            return _context2.abrupt(\"return\", new Promise(function (resolve) {\n              video.onloadedmetadata = function () {\n                resolve(video);\n              };\n            }));\n\n          case 10:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _setupCamera.apply(this, arguments);\n}\n\nfunction loadVideo() {\n  return _loadVideo.apply(this, arguments);\n}\n\nfunction _loadVideo() {\n  _loadVideo = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee3() {\n    var video;\n    return regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _context3.next = 2;\n            return setupCamera();\n\n          case 2:\n            video = _context3.sent;\n            video.play();\n            return _context3.abrupt(\"return\", video);\n\n          case 5:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n  return _loadVideo.apply(this, arguments);\n}\n\nvar defaultPoseNetArchitecture = 'MobileNetV1';\nvar defaultQuantBytes = 2;\nvar defaultMultiplier = 1.0;\nvar defaultStride = 16;\nvar defaultInputResolution = 200;\nvar guiState = {\n  avatarSVG: Object.keys(avatarSvgs)[0],\n  debug: {\n    showDetectionDebug: true,\n    showIllustrationDebug: false\n  }\n};\n/**\n * Sets up dat.gui controller on the top-right of the window\n */\n\nfunction setupGui(cameras) {\n  if (cameras.length > 0) {\n    guiState.camera = cameras[0].deviceId;\n  }\n\n  var gui = new _dat.default.GUI({\n    width: 300\n  });\n  var multi = gui.addFolder('Image');\n  gui.add(guiState, 'avatarSVG', Object.keys(avatarSvgs)).onChange(function () {\n    return parseSVG(avatarSvgs[guiState.avatarSVG]);\n  });\n  multi.open();\n  var output = gui.addFolder('Debug control');\n  output.add(guiState.debug, 'showDetectionDebug');\n  output.add(guiState.debug, 'showIllustrationDebug');\n  output.open();\n}\n/**\n * Sets up a frames per second panel on the top-left of the window\n */\n\n\nfunction setupFPS() {\n  stats.showPanel(0); // 0: fps, 1: ms, 2: mb, 3+: custom\n\n  document.getElementById('main').appendChild(stats.dom);\n}\n/**\n * Feeds an image to posenet to estimate poses - this is where the magic\n * happens. This function loops with a requestAnimationFrame method.\n */\n\n\nfunction detectPoseInRealTime(video) {\n  var canvas = document.getElementById('output');\n  var keypointCanvas = document.getElementById('keypoints');\n  var videoCtx = canvas.getContext('2d');\n  var keypointCtx = keypointCanvas.getContext('2d');\n  canvas.width = videoWidth;\n  canvas.height = videoHeight;\n  keypointCanvas.width = videoWidth;\n  keypointCanvas.height = videoHeight;\n\n  function poseDetectionFrame() {\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  function _poseDetectionFrame() {\n    _poseDetectionFrame = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee() {\n      var poses, input, all_poses, face;\n      return regeneratorRuntime.wrap(function _callee$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              // Begin monitoring code for frames per second\n              stats.begin();\n              poses = [];\n              videoCtx.clearRect(0, 0, videoWidth, videoHeight); // Draw video\n\n              videoCtx.save();\n              videoCtx.scale(-1, 1);\n              videoCtx.translate(-videoWidth, 0);\n              videoCtx.drawImage(video, 0, 0, videoWidth, videoHeight);\n              videoCtx.restore(); // Creates a tensor from an image\n\n              input = tf.browser.fromPixels(canvas);\n              _context.next = 11;\n              return facemesh.estimateFaces(input, false, false);\n\n            case 11:\n              faceDetection = _context.sent;\n              _context.next = 14;\n              return posenet.estimatePoses(video, {\n                flipHorizontal: true,\n                decodingMethod: 'multi-person',\n                maxDetections: 1,\n                scoreThreshold: minPartConfidence,\n                nmsRadius: nmsRadius\n              });\n\n            case 14:\n              all_poses = _context.sent;\n              poses = poses.concat(all_poses);\n              input.dispose();\n              keypointCtx.clearRect(0, 0, videoWidth, videoHeight);\n\n              if (guiState.debug.showDetectionDebug) {\n                poses.forEach(function (_ref) {\n                  var score = _ref.score,\n                      keypoints = _ref.keypoints;\n\n                  if (score >= minPoseConfidence) {\n                    (0, _demoUtils.drawKeypoints)(keypoints, minPartConfidence, keypointCtx);\n                    (0, _demoUtils.drawSkeleton)(keypoints, minPartConfidence, keypointCtx);\n                  }\n                });\n                faceDetection.forEach(function (face) {\n                  Object.values(_skeleton.facePartName2Index).forEach(function (index) {\n                    var p = face.scaledMesh[index];\n                    (0, _demoUtils.drawPoint)(keypointCtx, p[1], p[0], 2, 'red');\n                  });\n                });\n              }\n\n              canvasScope.project.clear();\n\n              if (poses.length >= 1 && illustration) {\n                _skeleton.Skeleton.flipPose(poses[0]);\n\n                if (faceDetection && faceDetection.length > 0) {\n                  face = _skeleton.Skeleton.toFaceFrame(faceDetection[0]);\n                  illustration.updateSkeleton(poses[0], face);\n                } else {\n                  illustration.updateSkeleton(poses[0], null);\n                }\n\n                illustration.draw(canvasScope, videoWidth, videoHeight);\n\n                if (guiState.debug.showIllustrationDebug) {\n                  illustration.debugDraw(canvasScope);\n                }\n              }\n\n              canvasScope.project.activeLayer.scale(canvasWidth / videoWidth, canvasHeight / videoHeight, new canvasScope.Point(0, 0)); // End monitoring code for frames per second\n\n              stats.end();\n              requestAnimationFrame(poseDetectionFrame);\n\n            case 24:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, _callee);\n    }));\n    return _poseDetectionFrame.apply(this, arguments);\n  }\n\n  poseDetectionFrame();\n}\n\nfunction setupCanvas() {\n  mobile = (0, _demoUtils.isMobile)();\n\n  if (mobile) {\n    canvasWidth = Math.min(window.innerWidth, window.innerHeight);\n    canvasHeight = canvasWidth;\n    videoWidth *= 0.7;\n    videoHeight *= 0.7;\n  }\n\n  canvasScope = paper.default;\n  var canvas = document.querySelector('.illustration-canvas');\n  ;\n  canvas.width = canvasWidth;\n  canvas.height = canvasHeight;\n  canvasScope.setup(canvas);\n}\n/**\n * Kicks off the demo by loading the posenet model, finding and loading\n * available camera devices, and setting off the detectPoseInRealTime function.\n */\n\n\nfunction bindPage() {\n  return _bindPage.apply(this, arguments);\n}\n\nfunction _bindPage() {\n  _bindPage = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee4() {\n    var t0, info, randomSVGSel;\n    return regeneratorRuntime.wrap(function _callee4$(_context4) {\n      while (1) {\n        switch (_context4.prev = _context4.next) {\n          case 0:\n            setupCanvas();\n            (0, _demoUtils.toggleLoadingUI)(true);\n            (0, _demoUtils.setStatusText)('Loading PoseNet model...');\n            _context4.next = 5;\n            return posenet_module.load({\n              architecture: defaultPoseNetArchitecture,\n              outputStride: defaultStride,\n              inputResolution: defaultInputResolution,\n              multiplier: defaultMultiplier,\n              quantBytes: defaultQuantBytes\n            });\n\n          case 5:\n            posenet = _context4.sent;\n            (0, _demoUtils.setStatusText)('Loading FaceMesh model...');\n            _context4.next = 9;\n            return facemesh_module.load();\n\n          case 9:\n            facemesh = _context4.sent;\n            (0, _demoUtils.setStatusText)('Loading Avatar file...');\n            t0 = new Date();\n            _context4.next = 14;\n            return parseSVG(Object.values(avatarSvgs)[0]);\n\n          case 14:\n            (0, _demoUtils.setStatusText)('Setting up camera...');\n            _context4.prev = 15;\n            _context4.next = 18;\n            return loadVideo();\n\n          case 18:\n            video = _context4.sent;\n            _context4.next = 27;\n            break;\n\n          case 21:\n            _context4.prev = 21;\n            _context4.t0 = _context4[\"catch\"](15);\n            info = document.getElementById('info');\n            info.textContent = 'this device type is not supported yet, ' + 'or this browser does not support video capture: ' + _context4.t0.toString();\n            info.style.display = 'block';\n            throw _context4.t0;\n\n          case 27:\n            // setupGui([], posenet);\n            // setupFPS();\n            // svgItems = Object.keys(avatarSvgs);\n            // parseSVG(avatarSvgs[1]);\n            randomSVGSel = getRandomInt(0, Object.values(avatarSvgs).length);\n            console.log(randomSVGSel);\n            parseSVG(Object.values(avatarSvgs)[randomSVGSel]);\n            console.log(\"gautam Testing\", avatarSvgs);\n            (0, _demoUtils.toggleLoadingUI)(false);\n            detectPoseInRealTime(video, posenet);\n\n          case 33:\n          case \"end\":\n            return _context4.stop();\n        }\n      }\n    }, _callee4, null, [[15, 21]]);\n  }));\n  return _bindPage.apply(this, arguments);\n}\n\nnavigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;\n\n_fileUtils.FileUtils.setDragDropHandler(function (result) {\n  parseSVG(result);\n});\n\nfunction parseSVG(_x) {\n  return _parseSVG.apply(this, arguments);\n}\n\nfunction _parseSVG() {\n  _parseSVG = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee5(target) {\n    var svgScope, skeleton;\n    return regeneratorRuntime.wrap(function _callee5$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            console.log(target);\n            _context5.next = 3;\n            return _svgUtils.SVGUtils.importSVG(target\n            /* SVG string or file path */\n            );\n\n          case 3:\n            svgScope = _context5.sent;\n            skeleton = new _skeleton.Skeleton(svgScope);\n            illustration = new _illustration.PoseIllustration(canvasScope);\n            illustration.bindSkeleton(skeleton, svgScope); // playAudio();\n\n          case 7:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee5);\n  }));\n  return _parseSVG.apply(this, arguments);\n}\n\nbindPage();"},"sourceMaps":null,"error":null,"hash":"b493965d330d7f0af0bef9e093b401da","cacheData":{"env":{}}}